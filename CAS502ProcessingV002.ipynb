{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOn80xb3t1pzIKDlMd9yuld",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CASAttackZW2025/CAS502Project/blob/main/CAS502ProcessingV002.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktjrki3i2qdE"
      },
      "outputs": [],
      "source": [
        "!pip install simpy\n",
        "\n",
        "import simpy\n",
        "import heapq\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import logging\n",
        "from collections import defaultdict, deque\n",
        "from itertools import combinations, product\n",
        "import time\n",
        "import datetime\n",
        "import calendar\n",
        "import csv\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ProcessSimulation:\n",
        "    \"\"\"\n",
        "    A manufacturing process simulation using SimPy.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        initial_entities,\n",
        "        unit_arrival_pattern,\n",
        "        paths_info,\n",
        "        p_level_changers=None,\n",
        "        path_changers=None,\n",
        "        priority_changers=None,\n",
        "        exclude_processors=None,\n",
        "        n_entities=100,\n",
        "        default_p_level=1,\n",
        "        default_entity_path='Path1',\n",
        "        default_priority=10,\n",
        "        top_n=10\n",
        "    ):\n",
        "        self.env = simpy.Environment()\n",
        "        self.initial_entities = initial_entities\n",
        "        self.unit_arrival_pattern = unit_arrival_pattern\n",
        "        self.paths_info = paths_info\n",
        "        self.p_level_changers = p_level_changers or {}\n",
        "        self.path_changers = path_changers or {}\n",
        "        self.priority_changers = priority_changers or {}\n",
        "        self.exclude_processors = exclude_processors or []\n",
        "        self.n_entities = n_entities\n",
        "        self.default_p_level = default_p_level\n",
        "        self.default_entity_path = default_entity_path\n",
        "        self.default_priority = default_priority\n",
        "        self.top_n = top_n\n",
        "        self.G_all = self.compose_graphs()\n",
        "        self.processor_capacities, self.processor_to_subprocessors, self.subprocessor_to_processor = self.set_processor_capacities()\n",
        "        self.processor_resources = {}\n",
        "        for proc, capacity in self.processor_capacities.items():\n",
        "            self.processor_resources[proc] = simpy.PriorityResource(self.env, capacity=capacity)\n",
        "        self.entity_attributes = {}\n",
        "        self.entity_processor_history = defaultdict(lambda: defaultdict(list))\n",
        "        self.start_times = defaultdict(dict)\n",
        "        self.finish_times = defaultdict(dict)\n",
        "        self.busy_periods = defaultdict(list)\n",
        "        self.entity_id_counter = 1\n",
        "        self.total_entities_generated = 0\n",
        "        self.results = None\n",
        "\n",
        "    def compose_graphs(self):\n",
        "        \"\"\"\n",
        "        Combine all path graphs into a single directed graph.\n",
        "        \"\"\"\n",
        "        G_all = nx.DiGraph()\n",
        "        for path_name, path_data in self.paths_info.items():\n",
        "            path_graph = path_data['graph']\n",
        "            G_all = nx.compose(G_all, path_graph)\n",
        "        return G_all\n",
        "\n",
        "    def set_processor_capacities(self):\n",
        "        \"\"\"\n",
        "        Set capacities for processors and create subprocessors for capacities > 1.\n",
        "        \"\"\"\n",
        "        processor_capacities = {}\n",
        "        processor_to_subprocessors = {}\n",
        "        subprocessor_to_processor = {}\n",
        "\n",
        "        for path_name, path_data in self.paths_info.items():\n",
        "            processing_times = path_data.get('processing_times', {})\n",
        "            for proc, proc_info in processing_times.items():\n",
        "                capacity = proc_info.get('capacity', 1)\n",
        "                processor_capacities[proc] = capacity\n",
        "                if capacity > 1:\n",
        "                    subprocessor_names = [f\"{proc}_{i+1}\" for i in range(capacity)]\n",
        "                    processor_to_subprocessors[proc] = subprocessor_names\n",
        "                    for subproc in subprocessor_names:\n",
        "                        subprocessor_to_processor[subproc] = proc\n",
        "                        processor_capacities[subproc] = 1\n",
        "                else:\n",
        "                    processor_to_subprocessors[proc] = [proc]\n",
        "                    subprocessor_to_processor[proc] = proc\n",
        "                    processor_capacities[proc] = 1\n",
        "\n",
        "        return processor_capacities, processor_to_subprocessors, subprocessor_to_processor\n",
        "\n",
        "    def get_processing_time(self, proc_info, current_p_level):\n",
        "        \"\"\"\n",
        "        Determine processing time based on current p-level.\n",
        "        \"\"\"\n",
        "        if 'processing_time' not in proc_info:\n",
        "            raise KeyError(\"proc_info must contain a 'processing_time' key.\")\n",
        "        processing_time = proc_info['processing_time']\n",
        "        if isinstance(processing_time, dict):\n",
        "            return processing_time.get(current_p_level, processing_time.get(1, 0))\n",
        "        return processing_time\n",
        "\n",
        "    def generate_entities(self):\n",
        "        \"\"\"\n",
        "        Generate all entities based on initial entities and arrival pattern.\n",
        "        \"\"\"\n",
        "        for initial_info in self.initial_entities:\n",
        "            arrival_time = initial_info.get('arrival_time', 0)\n",
        "            num_entities = initial_info['num_entities']\n",
        "            if self.total_entities_generated + num_entities > self.n_entities:\n",
        "                num_entities = self.n_entities - self.total_entities_generated\n",
        "            entity_paths = initial_info.get('entity_paths', [self.default_entity_path] * num_entities)\n",
        "            p_levels = initial_info.get('p_levels', [self.default_p_level] * num_entities)\n",
        "            priorities = initial_info.get('priorities', [self.default_priority] * num_entities)\n",
        "            for i in range(num_entities):\n",
        "                entity_id = self.entity_id_counter\n",
        "                self.entity_id_counter += 1\n",
        "                self.total_entities_generated += 1\n",
        "                self.entity_attributes[entity_id] = {\n",
        "                    'arrival_time': arrival_time,\n",
        "                    'path': entity_paths[i % len(entity_paths)],\n",
        "                    'p_level': p_levels[i % len(p_levels)],\n",
        "                    'priority': priorities[i % len(priorities)]\n",
        "                }\n",
        "                self.env.process(self.entity_arrival_process(entity_id, arrival_time))\n",
        "            if self.total_entities_generated >= self.n_entities:\n",
        "                break\n",
        "        if self.total_entities_generated < self.n_entities and self.unit_arrival_pattern:\n",
        "            repetition = 0\n",
        "            max_arrival_time = max(info['arrival_time'] for info in self.unit_arrival_pattern) if self.unit_arrival_pattern else 0\n",
        "            while self.total_entities_generated < self.n_entities:\n",
        "                for arrival_info in self.unit_arrival_pattern:\n",
        "                    arrival_time = arrival_info['arrival_time'] + repetition * (max_arrival_time + 1)\n",
        "                    num_entities = arrival_info['num_entities']\n",
        "                    if self.total_entities_generated + num_entities > self.n_entities:\n",
        "                        num_entities = self.n_entities - self.total_entities_generated\n",
        "                    entity_paths = arrival_info.get('entity_paths', [self.default_entity_path] * num_entities)\n",
        "                    p_levels = arrival_info.get('p_levels', [self.default_p_level] * num_entities)\n",
        "                    priorities = arrival_info.get('priorities', [self.default_priority] * num_entities)\n",
        "                    for i in range(num_entities):\n",
        "                        entity_id = self.entity_id_counter\n",
        "                        self.entity_id_counter += 1\n",
        "                        self.total_entities_generated += 1\n",
        "                        self.entity_attributes[entity_id] = {\n",
        "                            'arrival_time': arrival_time,\n",
        "                            'path': entity_paths[i % len(entity_paths)],\n",
        "                            'p_level': p_levels[i % len(p_levels)],\n",
        "                            'priority': priorities[i % len(priorities)]\n",
        "                        }\n",
        "                        self.env.process(self.entity_arrival_process(entity_id, arrival_time))\n",
        "                    if self.total_entities_generated >= self.n_entities:\n",
        "                        break\n",
        "                repetition += 1\n",
        "                if self.total_entities_generated >= self.n_entities:\n",
        "                    break\n",
        "\n",
        "    def entity_arrival_process(self, entity_id, arrival_time):\n",
        "        \"\"\"\n",
        "        Process representing an entity's arrival and path through the system.\n",
        "        \"\"\"\n",
        "        yield self.env.timeout(arrival_time)\n",
        "        entity = self.entity_attributes[entity_id]\n",
        "        entity_path = entity['path']\n",
        "        first_processors = [p for p in self.G_all.nodes if self.G_all.in_degree(p) == 0]\n",
        "        for proc in first_processors:\n",
        "            if proc in self.paths_info[entity_path]['processing_times']:\n",
        "                self.env.process(self.process_at_processor(entity_id, proc))\n",
        "\n",
        "    def process_at_processor(self, entity_id, proc):\n",
        "        \"\"\"\n",
        "        Process an entity at a specified processor and handle transitions to successors.\n",
        "        \"\"\"\n",
        "        entity = self.entity_attributes[entity_id]\n",
        "        entity_path = entity['path']\n",
        "        current_p_level = entity['p_level']\n",
        "        priority = entity['priority']\n",
        "        if proc not in self.paths_info[entity_path]['processing_times']:\n",
        "            return\n",
        "        proc_info = self.paths_info[entity_path]['processing_times'][proc]\n",
        "        subprocessors = self.processor_to_subprocessors.get(proc, [proc])\n",
        "        while True:\n",
        "            for subproc in subprocessors:\n",
        "                if not self.processor_resources[subproc].queue and self.processor_resources[subproc].count < self.processor_resources[subproc].capacity:\n",
        "                    resource = self.processor_resources[subproc]\n",
        "                    with resource.request(priority=priority) as req:\n",
        "                        yield req\n",
        "                        start_time = self.env.now\n",
        "                        self.start_times[subproc][entity_id] = start_time\n",
        "                        original_proc = self.subprocessor_to_processor.get(subproc, subproc)\n",
        "                        if original_proc not in self.entity_processor_history[entity_id]:\n",
        "                            self.entity_processor_history[entity_id][original_proc] = []\n",
        "                        self.entity_processor_history[entity_id][original_proc].append({'start_time': start_time})\n",
        "                        processing_time = self.get_processing_time(proc_info, current_p_level)\n",
        "                        yield self.env.timeout(processing_time)\n",
        "                        finish_time = self.env.now\n",
        "                        self.finish_times[subproc][entity_id] = finish_time\n",
        "                        self.entity_processor_history[entity_id][original_proc][-1]['end_time'] = finish_time\n",
        "                        self.busy_periods[subproc].append((start_time, finish_time))\n",
        "                        if original_proc in self.p_level_changers:\n",
        "                            entity['p_level'] = self.p_level_changers[original_proc](entity['p_level'])\n",
        "                        if original_proc in self.path_changers:\n",
        "                            entity['path'] = self.path_changers[original_proc](entity['path'])\n",
        "                        if original_proc in self.priority_changers:\n",
        "                            entity['priority'] = self.priority_changers[original_proc](entity['priority'])\n",
        "                        self.schedule_successor_processors(entity_id, original_proc)\n",
        "                    return\n",
        "            yield self.env.timeout(0.1)\n",
        "\n",
        "    def schedule_successor_processors(self, entity_id, proc):\n",
        "        \"\"\"\n",
        "        Determine and schedule next processors for the entity.\n",
        "        \"\"\"\n",
        "        entity = self.entity_attributes[entity_id]\n",
        "        entity_path = entity['path']\n",
        "        current_p_level = entity['p_level']\n",
        "        proc_info = self.paths_info[entity_path]['processing_times'].get(proc, {})\n",
        "        if 'p_level_successors' in proc_info:\n",
        "            possible_successors = proc_info['p_level_successors'].get(current_p_level, [])\n",
        "            for succ in possible_successors:\n",
        "                self.env.process(self.process_at_processor(entity_id, succ))\n",
        "        else:\n",
        "            for succ in self.G_all.successors(proc):\n",
        "                if succ in self.paths_info[entity_path]['processing_times']:\n",
        "                    preds = list(self.G_all.predecessors(succ))\n",
        "                    all_preds_finished = True\n",
        "                    for pred in preds:\n",
        "                        subprocs = self.processor_to_subprocessors.get(pred, [pred])\n",
        "                        pred_finished = False\n",
        "                        for subproc in subprocs:\n",
        "                            if entity_id in self.finish_times.get(subproc, {}):\n",
        "                                pred_finished = True\n",
        "                                break\n",
        "                        if not pred_finished:\n",
        "                            all_preds_finished = False\n",
        "                            break\n",
        "                    if all_preds_finished:\n",
        "                        self.env.process(self.process_at_processor(entity_id, succ))\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"\n",
        "        Run the simulation and collect results.\n",
        "        \"\"\"\n",
        "        self.generate_entities()\n",
        "        self.env.run()\n",
        "        total_time = 0\n",
        "        for proc in self.finish_times:\n",
        "            if self.finish_times[proc]:\n",
        "                total_time = max(total_time, max(self.finish_times[proc].values()))\n",
        "        idle_times, busy_times = self.calculate_idle_busy_times(total_time)\n",
        "        all_processors = {proc: {'busy_periods': self.busy_periods[proc]} for proc in self.busy_periods}\n",
        "        joint_times = self.compute_joint_times(all_processors, k=2, total_time=total_time)\n",
        "        top_idle_combinations = self.find_top_idle_combinations(joint_times, k=2, top_n=self.top_n)\n",
        "        self.results = {\n",
        "            'start_times': self.start_times,\n",
        "            'finish_times': self.finish_times,\n",
        "            'idle_times': idle_times,\n",
        "            'busy_times': busy_times,\n",
        "            'entity_attributes': self.entity_attributes,\n",
        "            'total_processing_time': total_time,\n",
        "            'busy_periods': self.busy_periods,\n",
        "            'joint_times': joint_times,\n",
        "            'paths_info': self.paths_info,\n",
        "            'G_all': self.G_all,\n",
        "            'entity_processor_history': self.entity_processor_history,\n",
        "            'top_idle_combinations': top_idle_combinations\n",
        "        }\n",
        "        return self.results\n",
        "\n",
        "    def calculate_idle_busy_times(self, total_time):\n",
        "        \"\"\"\n",
        "        Calculate idle and busy times for each processor.\n",
        "        \"\"\"\n",
        "        idle_times = defaultdict(float)\n",
        "        busy_times = defaultdict(float)\n",
        "        for proc in self.finish_times:\n",
        "            sorted_busy_periods = sorted(self.busy_periods[proc])\n",
        "            idle_periods = self.get_idle_periods(sorted_busy_periods, total_time)\n",
        "            idle_time = sum(end - start for start, end in idle_periods)\n",
        "            busy_time = sum(end - start for start, end in sorted_busy_periods)\n",
        "            idle_times[proc] = idle_time\n",
        "            busy_times[proc] = busy_time\n",
        "        return idle_times, busy_times\n",
        "\n",
        "    def get_idle_periods(self, busy_periods, total_time):\n",
        "        \"\"\"\n",
        "        Calculate idle periods based on busy periods.\n",
        "        \"\"\"\n",
        "        for period in busy_periods:\n",
        "            if not isinstance(period, tuple) or len(period) != 2:\n",
        "                raise ValueError(f\"Invalid busy period format: {period}. Expected a tuple of (start, end).\")\n",
        "            start, finish = period\n",
        "            if start > finish:\n",
        "                raise ValueError(f\"Invalid busy period times: {period}. Start time must be equal or less than end time.\")\n",
        "        idle_periods = []\n",
        "        prev_finish = 0\n",
        "        for start, finish in sorted(busy_periods):\n",
        "            if start > prev_finish:\n",
        "                idle_periods.append((prev_finish, start))\n",
        "            prev_finish = max(prev_finish, finish)\n",
        "        if prev_finish < total_time:\n",
        "            idle_periods.append((prev_finish, total_time))\n",
        "        return idle_periods\n",
        "\n",
        "    def intersect_intervals(self, intervals1, intervals2):\n",
        "        \"\"\"\n",
        "        Find overlapping intervals between two lists of time periods.\n",
        "        \"\"\"\n",
        "        for interval in intervals1 + intervals2:\n",
        "            if len(interval) != 2 or interval[0] >= interval[1]:\n",
        "                raise ValueError(f\"Invalid interval: {interval}. Each interval must be a tuple of (start, end) with start < end.\")\n",
        "        result = []\n",
        "        i, j = 0, 0\n",
        "        while i < len(intervals1) and j < len(intervals2):\n",
        "            a_start, a_end = intervals1[i]\n",
        "            b_start, b_end = intervals2[j]\n",
        "            start = max(a_start, b_start)\n",
        "            end = min(a_end, b_end)\n",
        "            if start < end:\n",
        "                result.append((start, end))\n",
        "            if a_end < b_end:\n",
        "                i += 1\n",
        "            else:\n",
        "                j += 1\n",
        "        return result\n",
        "\n",
        "    def compute_joint_times(self, all_processors, k, total_time):\n",
        "        \"\"\"\n",
        "        Compute joint idle/busy times for combinations of processors.\n",
        "        \"\"\"\n",
        "        joint_times = []\n",
        "        processor_list = [proc for proc in all_processors.keys() if proc not in self.exclude_processors]\n",
        "        for size in range(1, k + 1):\n",
        "            for combo in combinations(processor_list, size):\n",
        "                states = ['busy', 'idle']\n",
        "                for state_combo in product(states, repeat=size):\n",
        "                    proc_state = dict(zip(combo, state_combo))\n",
        "                    intervals = [(0, total_time)]\n",
        "                    for proc in combo:\n",
        "                        if proc_state[proc] == 'busy':\n",
        "                            periods = all_processors[proc]['busy_periods']\n",
        "                        else:\n",
        "                            periods = self.get_idle_periods(all_processors[proc]['busy_periods'], total_time)\n",
        "                        intervals = self.intersect_intervals(intervals, periods)\n",
        "                        if not intervals:\n",
        "                            break\n",
        "                    total_interval = sum(end - start for start, end in intervals)\n",
        "                    if total_interval > 0:\n",
        "                        joint_times.append({\n",
        "                            'processors': proc_state,\n",
        "                            'total_time': total_interval,\n",
        "                            'intervals': intervals\n",
        "                        })\n",
        "        return joint_times\n",
        "\n",
        "    def collect_all_processors(self, joint_times):\n",
        "        \"\"\"\n",
        "        Collect all unique processors from joint_times.\n",
        "        \"\"\"\n",
        "        all_procs = set()\n",
        "        for jt in joint_times:\n",
        "            all_procs.update(jt['processors'].keys())\n",
        "        return all_procs\n",
        "\n",
        "    def filter_processors(self, processor_list, exclude_processors):\n",
        "        \"\"\"\n",
        "        Filter out excluded processors from the processor list.\n",
        "        \"\"\"\n",
        "        exclude_set = set(exclude_processors)\n",
        "        logger.info(f\"Excluding processors: {exclude_set}\")\n",
        "        filtered_processors = [proc for proc in processor_list if proc not in exclude_set]\n",
        "        logger.info(f\"Filtered processors: {filtered_processors}\")\n",
        "        return filtered_processors\n",
        "\n",
        "    def compute_total_time(self, joint_times, combo, proc_state):\n",
        "        \"\"\"\n",
        "        Compute the total time where processors are in the specified states.\n",
        "        \"\"\"\n",
        "        total_time = 0.0\n",
        "        for jt in joint_times:\n",
        "            jt_procs = jt.get('processors', {})\n",
        "            jt_total_time = jt.get('total_time', 0.0)\n",
        "            if set(jt_procs.keys()) == set(combo):\n",
        "                if all(jt_procs[proc] == state for proc, state in proc_state.items()):\n",
        "                    total_time += jt_total_time\n",
        "        return total_time\n",
        "\n",
        "    def find_top_idle_combinations(self, joint_times, k, top_n=10):\n",
        "        \"\"\"\n",
        "        Find the top N combinations of processors with maximum joint idle time.\n",
        "        \"\"\"\n",
        "        combination_times = {}\n",
        "        all_procs = self.collect_all_processors(joint_times)\n",
        "        processor_list = self.filter_processors(all_procs, self.exclude_processors)\n",
        "        for size in range(2, k + 1):\n",
        "            for combo in combinations(processor_list, size):\n",
        "                for busy_proc in combo:\n",
        "                    proc_state = {proc: 'busy' if proc == busy_proc else 'idle' for proc in combo}\n",
        "                    total_time = self.compute_total_time(joint_times, combo, proc_state)\n",
        "                    if total_time > 0:\n",
        "                        key = (tuple(sorted(combo)), busy_proc)\n",
        "                        combination_times[key] = combination_times.get(key, 0) + total_time\n",
        "        sorted_combinations = sorted(combination_times.items(), key=lambda item: item[1], reverse=True)\n",
        "        top_combinations = sorted_combinations[:top_n]\n",
        "        result = []\n",
        "        for (procs, busy_proc), total_time in top_combinations:\n",
        "            result.append({\n",
        "                'processors': procs,\n",
        "                'busy_processor': busy_proc,\n",
        "                'total_time': total_time\n",
        "            })\n",
        "        return result\n",
        "\n",
        "    def plot_processing_schedule(self):\n",
        "        \"\"\"\n",
        "        Plot the processing schedule for each processor.\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            logger.warning(\"No results available. Run the simulation first.\")\n",
        "            return\n",
        "        entity_processor_history = self.results['entity_processor_history']\n",
        "        processors = set()\n",
        "        entities = set()\n",
        "        for entity_id in entity_processor_history:\n",
        "            entities.add(entity_id)\n",
        "            processors.update(entity_processor_history[entity_id].keys())\n",
        "        entities = sorted(entities)\n",
        "        n_entities = len(entities)\n",
        "        processor_start_times = {}\n",
        "        for processor in processors:\n",
        "            earliest_start = float('inf')\n",
        "            for entity_id in entity_processor_history:\n",
        "                if processor in entity_processor_history[entity_id]:\n",
        "                    for visit in entity_processor_history[entity_id][processor]:\n",
        "                        s = visit['start_time']\n",
        "                        if s < earliest_start:\n",
        "                            earliest_start = s\n",
        "            processor_start_times[processor] = earliest_start\n",
        "        processors = [p for p in processors if p != \"Start_Job\"]\n",
        "        sorted_processors = sorted(processors, key=lambda p: processor_start_times[p])\n",
        "        entity_colors = self.generate_random_colors(n_entities)\n",
        "        entity_color_dict = {entity_id: entity_colors[idx] for idx, entity_id in enumerate(entities)}\n",
        "        processor_colors = plt.cm.get_cmap('tab20', len(sorted_processors))\n",
        "        processor_color_dict = {processor: processor_colors(idx % 20) for idx, processor in enumerate(sorted_processors)}\n",
        "        fig, ax = plt.subplots(figsize=(14, 8))\n",
        "        yticks = []\n",
        "        ytick_labels = []\n",
        "        y_base = 0\n",
        "        for processor in sorted_processors:\n",
        "            visits = []\n",
        "            for entity_id in entity_processor_history:\n",
        "                if processor in entity_processor_history[entity_id]:\n",
        "                    for visit in entity_processor_history[entity_id][processor]:\n",
        "                        s = visit['start_time']\n",
        "                        f = visit.get('end_time', visit['start_time'])\n",
        "                        visits.append({'entity_id': entity_id, 'start_time': s, 'end_time': f})\n",
        "            visits.sort(key=lambda v: v['start_time'])\n",
        "            capacity = self.processor_capacities.get(processor, 1)\n",
        "            lanes = [[] for _ in range(capacity)]\n",
        "            for visit in visits:\n",
        "                placed = False\n",
        "                for lane in lanes:\n",
        "                    if not lane or visit['start_time'] >= lane[-1]['end_time']:\n",
        "                        lane.append(visit)\n",
        "                        placed = True\n",
        "                        break\n",
        "                if not placed:\n",
        "                    lanes.append([visit])\n",
        "            num_lanes = len(lanes)\n",
        "            for lane_idx, lane in enumerate(lanes):\n",
        "                for visit in lane:\n",
        "                    s = visit['start_time']\n",
        "                    f = visit['end_time']\n",
        "                    entity_id = visit['entity_id']\n",
        "                    entity_color = entity_color_dict[entity_id]\n",
        "                    processor_color = processor_color_dict[processor]\n",
        "                    if n_entities < 22:\n",
        "                        ax.broken_barh(\n",
        "                            [(s, f - s)],\n",
        "                            (y_base + lane_idx - 0.4, 1.3),\n",
        "                            facecolors=processor_color,\n",
        "                            edgecolors='black',\n",
        "                            linewidth=1\n",
        "                        )\n",
        "                        ax.broken_barh(\n",
        "                            [(s, f - s)],\n",
        "                            (y_base + lane_idx, 0.4),\n",
        "                            facecolors=entity_color,\n",
        "                            alpha=1,\n",
        "                            edgecolors='black',\n",
        "                            linewidth=1\n",
        "                        )\n",
        "                    else:\n",
        "                        ax.broken_barh(\n",
        "                            [(s, f - s)],\n",
        "                            (y_base + lane_idx - 0.4, 1.3),\n",
        "                            facecolors=processor_color,\n",
        "                            edgecolors='none',\n",
        "                            linewidth=1\n",
        "                        )\n",
        "                        ax.broken_barh(\n",
        "                            [(s, f - s)],\n",
        "                            (y_base + lane_idx, 0.4),\n",
        "                            facecolors=entity_color,\n",
        "                            alpha=1,\n",
        "                            edgecolors='none'\n",
        "                        )\n",
        "                    if n_entities < 11:\n",
        "                        ax.text(\n",
        "                            s + (f - s) / 2,\n",
        "                            y_base + lane_idx + 0.1,\n",
        "                            f'{entity_id}',\n",
        "                            ha='center',\n",
        "                            va='center',\n",
        "                            fontsize=8,\n",
        "                            color='white'\n",
        "                        )\n",
        "            ytick_pos = y_base + (num_lanes - 1) / 2\n",
        "            yticks.append(ytick_pos)\n",
        "            ytick_labels.append(f\"{processor}\")\n",
        "            y_base += num_lanes + 1\n",
        "        ax.set_xlabel('Time (hours)')\n",
        "        ax.set_ylabel('Processors')\n",
        "        ax.set_yticks(yticks)\n",
        "        ax.set_yticklabels(ytick_labels)\n",
        "        ax.set_title('Processing Schedule of Entities at Each Processor')\n",
        "        ax.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
        "        from matplotlib.patches import Patch\n",
        "        processor_patches = [\n",
        "            Patch(facecolor=processor_color_dict[processor], edgecolor='black', label=f'{processor}')\n",
        "            for processor in sorted_processors\n",
        "        ]\n",
        "        ax.legend(handles=processor_patches, title='Processors', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def generate_random_colors(self, num_colors):\n",
        "        \"\"\"\n",
        "        Generate random colors for visualization.\n",
        "        \"\"\"\n",
        "        np.random.seed(42)\n",
        "        colors = []\n",
        "        for _ in range(num_colors):\n",
        "            hue = np.random.rand()\n",
        "            saturation = 0.5 + 0.5 * np.random.rand()\n",
        "            value = 0.5 + 0.5 * np.random.rand()\n",
        "            from matplotlib import colors as mcolors\n",
        "            rgb_color = mcolors.hsv_to_rgb((hue, saturation, value))\n",
        "            colors.append(rgb_color)\n",
        "        return colors\n",
        "\n",
        "    def plot_results(self):\n",
        "        \"\"\"\n",
        "        Plot the arrival times and finish times of entities at the last processor of each path.\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            logger.warning(\"No results available. Run the simulation first.\")\n",
        "            return\n",
        "        entity_attributes = self.results['entity_attributes']\n",
        "        finish_times = self.results['finish_times']\n",
        "        paths_info = self.results['paths_info']\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        markers = ['o', 's', '^', 'D', 'v', 'P', '*', 'X', '+', '<', '>']\n",
        "        entities = list(entity_attributes.keys())\n",
        "        entities.sort()\n",
        "        num_entities = len(entities)\n",
        "        marker_idx = 0\n",
        "        color_cycle = plt.cm.get_cmap('tab20')\n",
        "        for idx, (path_name, path_data) in enumerate(paths_info.items()):\n",
        "            path_entities = [eid for eid, attr in entity_attributes.items() if attr['path'] == path_name]\n",
        "            if not path_entities:\n",
        "                continue\n",
        "            arrival_times = [entity_attributes[eid]['arrival_time'] for eid in path_entities]\n",
        "            if num_entities < 49:\n",
        "                plt.plot(\n",
        "                    path_entities,\n",
        "                    arrival_times,\n",
        "                    label=f'Arrival Times {path_name}',\n",
        "                    marker=markers[marker_idx % len(markers)],\n",
        "                    linestyle='--',\n",
        "                    color=color_cycle(idx % 20)\n",
        "                )\n",
        "            else:\n",
        "                plt.plot(\n",
        "                    path_entities,\n",
        "                    arrival_times,\n",
        "                    label=f'Arrival Times {path_name}',\n",
        "                    linestyle='--',\n",
        "                    color=color_cycle(idx % 20)\n",
        "                )\n",
        "            marker_idx += 1\n",
        "            G = path_data['graph']\n",
        "            try:\n",
        "                processors = list(nx.topological_sort(G))\n",
        "            except nx.NetworkXUnfeasible:\n",
        "                processors = list(G.nodes())\n",
        "            last_processor = None\n",
        "            for proc in reversed(processors):\n",
        "                if proc in path_data['processing_times']:\n",
        "                    last_processor = proc\n",
        "                    break\n",
        "            if last_processor:\n",
        "                subprocessors = self.processor_to_subprocessors.get(last_processor, [last_processor])\n",
        "                finish_times_last_proc = []\n",
        "                for eid in path_entities:\n",
        "                    finish_time = None\n",
        "                    for subproc in subprocessors:\n",
        "                        if eid in finish_times.get(subproc, {}):\n",
        "                            finish_time = finish_times[subproc][eid]\n",
        "                            break\n",
        "                    finish_times_last_proc.append(finish_time if finish_time is not None else np.nan)\n",
        "                if num_entities < 49:\n",
        "                    plt.plot(\n",
        "                        path_entities,\n",
        "                        finish_times_last_proc,\n",
        "                        label=f'Finish Times at {last_processor}',\n",
        "                        marker=markers[marker_idx % len(markers)],\n",
        "                        linestyle='-',\n",
        "                        color=color_cycle((idx + 1) % 20)\n",
        "                    )\n",
        "                else:\n",
        "                    plt.plot(\n",
        "                        path_entities,\n",
        "                        finish_times_last_proc,\n",
        "                        label=f'Finish Times at {last_processor}',\n",
        "                        linestyle='-',\n",
        "                        color=color_cycle((idx + 1) % 20)\n",
        "                    )\n",
        "                marker_idx += 1\n",
        "        plt.xlabel('Entity ID')\n",
        "        plt.ylabel('Time (hours)')\n",
        "        plt.title('Processing Times of Entities at Last Processor of Each Path')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def plot_results_individual_processors(self):\n",
        "        \"\"\"\n",
        "        Plot the arrival times and finish times of entities at each processor.\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            logger.warning(\"No results available. Run the simulation first.\")\n",
        "            return\n",
        "        entity_attributes = self.results['entity_attributes']\n",
        "        finish_times = self.results['finish_times']\n",
        "        paths_info = self.results['paths_info']\n",
        "        plt.figure(figsize=(14, 8))\n",
        "        markers = ['o', 's', '^', 'D', 'v', 'P', '*', 'X', '+', '<', '>']\n",
        "        color_cycle = plt.cm.get_cmap('tab20')\n",
        "        entities = list(entity_attributes.keys())\n",
        "        entities.sort()\n",
        "        num_entities = len(entities)\n",
        "        marker_idx = 0\n",
        "        for idx, (path_name, path_data) in enumerate(paths_info.items()):\n",
        "            path_entities = [eid for eid, attr in entity_attributes.items() if attr['path'] == path_name]\n",
        "            if not path_entities:\n",
        "                continue\n",
        "            arrival_times = [entity_attributes[eid]['arrival_time'] for eid in path_entities]\n",
        "            if num_entities < 49:\n",
        "                plt.plot(\n",
        "                    path_entities,\n",
        "                    arrival_times,\n",
        "                    label=f'Arrival Times {path_name}',\n",
        "                    marker=markers[marker_idx % len(markers)],\n",
        "                    linestyle='--',\n",
        "                    color=color_cycle(idx % 20)\n",
        "                )\n",
        "            else:\n",
        "                plt.plot(\n",
        "                    path_entities,\n",
        "                    arrival_times,\n",
        "                    label=f'Arrival Times {path_name}',\n",
        "                    linestyle='--',\n",
        "                    color=color_cycle(idx % 20)\n",
        "                )\n",
        "            marker_idx += 1\n",
        "            G = path_data['graph']\n",
        "            try:\n",
        "                processors = list(nx.topological_sort(G))\n",
        "            except nx.NetworkXUnfeasible:\n",
        "                processors = list(G.nodes())\n",
        "            for proc_idx, processor in enumerate(processors):\n",
        "                if processor in path_data['processing_times']:\n",
        "                    subprocessors = self.processor_to_subprocessors.get(processor, [processor])\n",
        "                    finish_times_proc = []\n",
        "                    for eid in path_entities:\n",
        "                        finish_time = None\n",
        "                        for subproc in subprocessors:\n",
        "                            if eid in finish_times.get(subproc, {}):\n",
        "                                finish_time = finish_times[subproc][eid]\n",
        "                                break\n",
        "                        finish_times_proc.append(finish_time if finish_time is not None else np.nan)\n",
        "                    if num_entities < 49:\n",
        "                        plt.plot(\n",
        "                            path_entities,\n",
        "                            finish_times_proc,\n",
        "                            label=f'Finish Times at {processor}',\n",
        "                            marker=markers[marker_idx % len(markers)],\n",
        "                            linestyle='-',\n",
        "                            color=color_cycle((idx + proc_idx + 1) % 20)\n",
        "                        )\n",
        "                    else:\n",
        "                        plt.plot(\n",
        "                            path_entities,\n",
        "                            finish_times_proc,\n",
        "                            label=f'Finish Times at {processor}',\n",
        "                            linestyle='-',\n",
        "                            color=color_cycle((idx + proc_idx + 1) % 20)\n",
        "                        )\n",
        "                    marker_idx += 1\n",
        "        plt.xlabel('Entity ID')\n",
        "        plt.ylabel('Time (hours)')\n",
        "        plt.title('Processing Times of Entities at Each Processor')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "    def save_joint_times_to_csv(self, output_file='idle_combinations.csv'):\n",
        "        \"\"\"\n",
        "        Save joint idle/busy times to a CSV file with dynamically determined headers.\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            logger.warning(\"No results available. Run the simulation first.\")\n",
        "            return\n",
        "        joint_times = self.results['joint_times']\n",
        "        if not joint_times:\n",
        "            print(\"No joint times to save.\")\n",
        "            return\n",
        "        try:\n",
        "            max_busy = max(len([proc for proc, state in jt['processors'].items() if state == 'busy']) for jt in joint_times)\n",
        "            max_idle = max(len([proc for proc, state in jt['processors'].items() if state == 'idle']) for jt in joint_times)\n",
        "            max_total = max(len(jt['processors']) for jt in joint_times)\n",
        "            fieldnames = [\n",
        "                'Processors',\n",
        "                'Busy Processors',\n",
        "                'Idle Processors',\n",
        "                'Total Time'\n",
        "            ]\n",
        "            fieldnames += [f'Busy Processor{i+1}' for i in range(max_busy)]\n",
        "            fieldnames += [f'Idle Processor{i+1}' for i in range(max_idle)]\n",
        "            fieldnames += [f'Processor{i+1}' for i in range(max_total)]\n",
        "            with open(output_file, 'w', newline='') as csvfile:\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                for jt in joint_times:\n",
        "                    processors = jt['processors']\n",
        "                    busy_processors = sorted([proc for proc, state in processors.items() if state == 'busy'])\n",
        "                    idle_processors = sorted([proc for proc, state in processors.items() if state == 'idle'])\n",
        "                    total_time = jt['total_time']\n",
        "                    if busy_processors and idle_processors:\n",
        "                        row = {\n",
        "                            'Processors': ', '.join(sorted(processors.keys())),\n",
        "                            'Busy Processors': ', '.join(busy_processors),\n",
        "                            'Idle Processors': ', '.join(idle_processors),\n",
        "                            'Total Time': f\"{total_time:.2f}\"\n",
        "                        }\n",
        "                        for i in range(max_busy):\n",
        "                            key = f'Busy Processor{i+1}'\n",
        "                            row[key] = busy_processors[i] if i < len(busy_processors) else ''\n",
        "                        for i in range(max_idle):\n",
        "                            key = f'Idle Processor{i+1}'\n",
        "                            row[key] = idle_processors[i] if i < len(idle_processors) else ''\n",
        "                        sorted_procs = sorted(processors.keys())\n",
        "                        for i in range(max_total):\n",
        "                            key = f'Processor{i+1}'\n",
        "                            row[key] = sorted_procs[i] if i < len(sorted_procs) else ''\n",
        "                        writer.writerow(row)\n",
        "            print(f\"Joint times CSV file '{output_file}' has been created successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while saving the joint times CSV file: {e}\")\n",
        "\n",
        "    def clean_idle_combinations_csv(self, input_file, output_file, excluded_processors):\n",
        "        \"\"\"\n",
        "        Remove rows from the idle_combinations.csv that contain any excluded processors.\n",
        "        \"\"\"\n",
        "        excluded_set = set(excluded_processors)\n",
        "        try:\n",
        "            with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
        "                reader = csv.DictReader(infile)\n",
        "                fieldnames = reader.fieldnames\n",
        "                writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                for row in reader:\n",
        "                    processors_in_row = set(proc.strip() for proc in row['Processors'].split(','))\n",
        "                    if excluded_set.intersection(processors_in_row):\n",
        "                        continue\n",
        "                    writer.writerow(row)\n",
        "            print(f\"Cleaned CSV saved to '{output_file}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while cleaning the CSV file: {e}\")\n",
        "\n",
        "    def save_processor_schedule_and_dependencies_to_csv(self, output_file='processor_schedules_dependencies.csv'):\n",
        "        \"\"\"\n",
        "        Save processor schedules and dependencies to a CSV file.\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            logger.warning(\"No results available. Run the simulation first.\")\n",
        "            return\n",
        "        try:\n",
        "            with open(output_file, 'w', newline='') as csvfile:\n",
        "                fieldnames = ['Processor', 'Predecessors', 'Successors', 'Entity_ID', 'Start_Time', 'End_Time']\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                G_all = self.results['G_all']\n",
        "                entity_history = self.results['entity_processor_history']\n",
        "                for proc in G_all.nodes:\n",
        "                    predecessors = list(G_all.predecessors(proc))\n",
        "                    successors = list(G_all.successors(proc))\n",
        "                    for entity_id, proc_history in entity_history.items():\n",
        "                        if proc in proc_history:\n",
        "                            visits = proc_history[proc]\n",
        "                            for visit in visits:\n",
        "                                start_time = visit.get('start_time', '')\n",
        "                                end_time = visit.get('end_time', '')\n",
        "                                writer.writerow({\n",
        "                                    'Processor': proc,\n",
        "                                    'Predecessors': ';'.join(predecessors),\n",
        "                                    'Successors': ';'.join(successors),\n",
        "                                    'Entity_ID': entity_id,\n",
        "                                    'Start_Time': start_time,\n",
        "                                    'End_Time': end_time\n",
        "                                })\n",
        "            print(f\"Processor schedules and dependencies have been saved to '{output_file}'.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while saving the processor schedules and dependencies CSV file: {e}\")\n",
        "\n",
        "    def construct_optimal_pools(self, file_path, k, m, excluded_processors=None):\n",
        "        \"\"\"\n",
        "        Construct optimal processor pools based on idle times from a CSV file.\n",
        "        \"\"\"\n",
        "        excluded_processors = excluded_processors or []\n",
        "        excluded_set = set(excluded_processors)\n",
        "        processor_idle_times = defaultdict(float)\n",
        "        pools = defaultdict(list)\n",
        "        try:\n",
        "            with open(file_path, 'r') as file:\n",
        "                reader = csv.DictReader(file)\n",
        "                for row in reader:\n",
        "                    processors_in_row = tuple(sorted(proc.strip() for proc in row['Processors'].split(',')))\n",
        "                    if excluded_set.intersection(processors_in_row):\n",
        "                        continue\n",
        "                    try:\n",
        "                        busy_processors = [proc.strip() for proc in row['Busy Processors'].split(',') if proc.strip()]\n",
        "                    except KeyError:\n",
        "                        busy_processors = []\n",
        "                    if excluded_set.intersection(busy_processors):\n",
        "                        continue\n",
        "                    try:\n",
        "                        total_time = float(row['Total Time'])\n",
        "                    except ValueError:\n",
        "                        total_time = 0.0\n",
        "                    processor_idle_times[processors_in_row] += total_time\n",
        "                    for busy_processor in busy_processors:\n",
        "                        pools[processors_in_row].append((busy_processor, total_time))\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while reading the CSV file: {e}\")\n",
        "            return []\n",
        "        sum_pools = defaultdict(float)\n",
        "        for processors, times in pools.items():\n",
        "            for busy_processor, total_time in times:\n",
        "                subset = tuple(sorted(set(processors) - {busy_processor}))\n",
        "                sum_pools[processors] += total_time\n",
        "                if subset:\n",
        "                    sum_pools[subset] += total_time\n",
        "        def compute_idle_times(processors, depth, current_time):\n",
        "            total_idle_time = current_time\n",
        "            for i in range(1, len(processors)):\n",
        "                for subset in combinations(processors, i):\n",
        "                    if subset in sum_pools:\n",
        "                        total_idle_time += compute_idle_times(subset, depth + 1, sum_pools[subset])\n",
        "            return total_idle_time\n",
        "        optimal_pools = []\n",
        "        for processors in sum_pools.keys():\n",
        "            if len(processors) <= k:\n",
        "                total_idle_time = compute_idle_times(processors, 0, sum_pools[processors])\n",
        "                optimal_pools.append((processors, total_idle_time))\n",
        "        optimal_pools.sort(key=lambda x: x[1], reverse=True)\n",
        "        return optimal_pools[:m]\n",
        "\n",
        "    def save_optimal_pools_to_csv(self, optimal_pools, output_file='optimal_pools.csv'):\n",
        "        \"\"\"\n",
        "        Save optimal pools to a CSV file.\n",
        "        \"\"\"\n",
        "        if not optimal_pools:\n",
        "            print(\"No optimal pools to save.\")\n",
        "            return\n",
        "        try:\n",
        "            max_processors = max(len(pool[0]) for pool in optimal_pools)\n",
        "            fieldnames = ['Pool', 'Total Idle Time']\n",
        "            fieldnames += [f'Processor{i+1}' for i in range(max_processors)]\n",
        "            with open(output_file, 'w', newline='') as csvfile:\n",
        "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "                for i, (processors, total_idle_time) in enumerate(optimal_pools, start=1):\n",
        "                    row = {\n",
        "                        'Pool': i,\n",
        "                        'Total Idle Time': f\"{total_idle_time:.2f} hours\"\n",
        "                    }\n",
        "                    for j, processor in enumerate(processors):\n",
        "                        row[f'Processor{j+1}'] = processor\n",
        "                    for j in range(len(processors), max_processors):\n",
        "                        row[f'Processor{j+1}'] = ''\n",
        "                    writer.writerow(row)\n",
        "            print(f\"CSV file '{output_file}' has been created successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while saving the CSV file: {e}\")\n",
        "\n",
        "    def create_entity_arrival_schedule(\n",
        "        self,\n",
        "        start_date: str,\n",
        "        interval_type: str,\n",
        "        end_date: str,\n",
        "        initial_num_entities: int,\n",
        "        arrival_pattern_num_entities: int,\n",
        "        remove_weekends: bool = False,\n",
        "        holiday_dates: Optional[List[str]] = None,\n",
        "        subtract_pto_hours_per_week: float = 0,\n",
        "        scale_days: float = 24,\n",
        "        remove_holiday_overlap: bool = True\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Create a schedule of entity arrivals based on specified parameters.\n",
        "        \"\"\"\n",
        "        start_date_dt = datetime.datetime.strptime(start_date, '%Y-%m-%d').date()\n",
        "        end_date_dt = datetime.datetime.strptime(end_date, '%Y-%m-%d').date()\n",
        "        if holiday_dates:\n",
        "            holiday_dates_dt = [datetime.datetime.strptime(date, '%Y-%m-%d').date() for date in holiday_dates]\n",
        "        else:\n",
        "            holiday_dates_dt = []\n",
        "        initial_entities = [{\n",
        "            'arrival_time': 0,\n",
        "            'num_entities': initial_num_entities\n",
        "        }]\n",
        "        unit_arrival_pattern = []\n",
        "        def add_interval(date: datetime.date, interval: str) -> datetime.date:\n",
        "            if interval == 'daily':\n",
        "                return date + datetime.timedelta(days=1)\n",
        "            elif interval == 'weekly':\n",
        "                return date + datetime.timedelta(weeks=1)\n",
        "            elif interval == 'monthly':\n",
        "                month = date.month\n",
        "                year = date.year + month // 12\n",
        "                month = month % 12 + 1\n",
        "                day = min(date.day, calendar.monthrange(year, month)[1])\n",
        "                return datetime.date(year, month, day)\n",
        "            elif interval == 'quarterly':\n",
        "                month = date.month + 3\n",
        "                year = date.year + (month - 1) // 12\n",
        "                month = (month - 1) % 12 + 1\n",
        "                day = min(date.day, calendar.monthrange(year, month)[1])\n",
        "                return datetime.date(year, month, day)\n",
        "            elif interval == 'yearly':\n",
        "                try:\n",
        "                    return datetime.date(date.year + 1, date.month, date.day)\n",
        "                except ValueError:\n",
        "                    return datetime.date(date.year + 1, date.month, 28)\n",
        "            else:\n",
        "                raise ValueError(\"Invalid interval type. Choose from 'daily', 'weekly', 'monthly', 'quarterly', 'yearly'.\")\n",
        "        def is_weekend(date: datetime.date) -> bool:\n",
        "            return date.weekday() >= 5\n",
        "        def calculate_total_hours(date: datetime.date, interval: str) -> float:\n",
        "            if interval == 'daily':\n",
        "                return scale_days\n",
        "            elif interval == 'weekly':\n",
        "                return scale_days * 7\n",
        "            elif interval == 'monthly':\n",
        "                _, num_days = calendar.monthrange(date.year, date.month)\n",
        "                return scale_days * num_days\n",
        "            elif interval == 'quarterly':\n",
        "                total_hours = 0\n",
        "                for m in range(0, 3):\n",
        "                    month = date.month + m\n",
        "                    year = date.year + (month - 1) // 12\n",
        "                    month = (month - 1) % 12 + 1\n",
        "                    _, num_days = calendar.monthrange(year, month)\n",
        "                    total_hours += scale_days * num_days\n",
        "                return total_hours\n",
        "            elif interval == 'yearly':\n",
        "                return scale_days * 366 if calendar.isleap(date.year) else scale_days * 365\n",
        "            else:\n",
        "                return scale_days\n",
        "        def calculate_weekend_hours(date: datetime.date, interval: str) -> float:\n",
        "            weekend_hours = 0\n",
        "            if interval == 'daily':\n",
        "                if is_weekend(date):\n",
        "                    weekend_hours += scale_days\n",
        "            elif interval == 'weekly':\n",
        "                weekend_hours += scale_days * 2\n",
        "            elif interval in ['monthly', 'quarterly', 'yearly']:\n",
        "                temp_date = date\n",
        "                next_date = add_interval(temp_date, interval)\n",
        "                while temp_date < next_date:\n",
        "                    if is_weekend(temp_date):\n",
        "                        weekend_hours += scale_days\n",
        "                    temp_date += datetime.timedelta(days=1)\n",
        "            return weekend_hours\n",
        "        def calculate_holiday_hours(date: datetime.date, interval: str) -> float:\n",
        "            holiday_hours = 0\n",
        "            if not holiday_dates_dt:\n",
        "                return holiday_hours\n",
        "            if interval == 'daily':\n",
        "                if date in holiday_dates_dt:\n",
        "                    holiday_hours += scale_days\n",
        "            else:\n",
        "                next_date = add_interval(date, interval)\n",
        "                for holiday in holiday_dates_dt:\n",
        "                    if date <= holiday < next_date:\n",
        "                        if remove_holiday_overlap or not is_weekend(holiday):\n",
        "                            holiday_hours += scale_days\n",
        "            return holiday_hours\n",
        "        def calculate_pto_hours(interval: str) -> float:\n",
        "            if subtract_pto_hours_per_week <= 0:\n",
        "                return 0\n",
        "            if interval == 'daily':\n",
        "                return (subtract_pto_hours_per_week / 7) * (scale_days / 24)\n",
        "            elif interval == 'weekly':\n",
        "                return subtract_pto_hours_per_week * (scale_days / 24)\n",
        "            elif interval == 'monthly':\n",
        "                weeks_in_month = 4.345\n",
        "                return subtract_pto_hours_per_week * weeks_in_month * (scale_days / 24)\n",
        "            elif interval == 'quarterly':\n",
        "                weeks_in_quarter = 13\n",
        "                return subtract_pto_hours_per_week * weeks_in_quarter * (scale_days / 24)\n",
        "            elif interval == 'yearly':\n",
        "                weeks_in_year = 52\n",
        "                return subtract_pto_hours_per_week * weeks_in_year * (scale_days / 24)\n",
        "            else:\n",
        "                return 0\n",
        "        current_date = start_date_dt\n",
        "        cumulative_hours = 0\n",
        "        total_adjusted_hours = 0\n",
        "        total_hours = calculate_total_hours(current_date, interval_type)\n",
        "        adjusted_hours = total_hours\n",
        "        if remove_weekends:\n",
        "            weekend_hours = calculate_weekend_hours(current_date, interval_type)\n",
        "            adjusted_hours -= weekend_hours\n",
        "        holiday_hours = calculate_holiday_hours(current_date, interval_type)\n",
        "        adjusted_hours -= holiday_hours\n",
        "        pto_hours = calculate_pto_hours(interval_type)\n",
        "        adjusted_hours -= pto_hours\n",
        "        adjusted_hours = max(adjusted_hours, 0)\n",
        "        cumulative_hours += adjusted_hours\n",
        "        current_date = add_interval(current_date, interval_type)\n",
        "        while current_date <= end_date_dt:\n",
        "            total_hours = calculate_total_hours(current_date, interval_type)\n",
        "            adjusted_hours = total_hours\n",
        "            if remove_weekends:\n",
        "                weekend_hours = calculate_weekend_hours(current_date, interval_type)\n",
        "                adjusted_hours -= weekend_hours\n",
        "            holiday_hours = calculate_holiday_hours(current_date, interval_type)\n",
        "            adjusted_hours -= holiday_hours\n",
        "            pto_hours = calculate_pto_hours(interval_type)\n",
        "            adjusted_hours -= pto_hours\n",
        "            adjusted_hours = max(adjusted_hours, 0)\n",
        "            unit_arrival_pattern.append({\n",
        "                'arrival_time': cumulative_hours,\n",
        "                'num_entities': arrival_pattern_num_entities\n",
        "            })\n",
        "            cumulative_hours += adjusted_hours\n",
        "            total_adjusted_hours += adjusted_hours\n",
        "            if end_date_dt <= add_interval(current_date, interval_type):\n",
        "                new_date2 = current_date\n",
        "                new_date1 = current_date\n",
        "                while new_date2 <= end_date_dt:\n",
        "                    interval_type1 = interval_type\n",
        "                    new_date1 = add_interval(new_date1, interval_type)\n",
        "                    if new_date1 > end_date_dt:\n",
        "                        interval_type1 = 'quarterly'\n",
        "                        new_date1 = add_interval(new_date1, interval_type1)\n",
        "                        if new_date1 > end_date_dt:\n",
        "                            interval_type1 = 'monthly'\n",
        "                            new_date1 = add_interval(new_date1, interval_type1)\n",
        "                            if new_date1 > end_date_dt:\n",
        "                                interval_type1 = 'weekly'\n",
        "                                new_date1 = add_interval(new_date1, interval_type1)\n",
        "                                if new_date1 > end_date_dt:\n",
        "                                    interval_type1 = 'daily'\n",
        "                                    new_date1 = add_interval(new_date1, interval_type1)\n",
        "                    total_hours = calculate_total_hours(new_date1, interval_type1)\n",
        "                    adjusted_hours = total_hours\n",
        "                    if remove_weekends:\n",
        "                        weekend_hours = calculate_weekend_hours(new_date1, interval_type1)\n",
        "                        adjusted_hours -= weekend_hours\n",
        "                    holiday_hours = calculate_holiday_hours(new_date1, interval_type1)\n",
        "                    adjusted_hours -= holiday_hours\n",
        "                    pto_hours = calculate_pto_hours(interval_type1)\n",
        "                    adjusted_hours -= pto_hours\n",
        "                    adjusted_hours = max(adjusted_hours, 0)\n",
        "                    total_adjusted_hours += adjusted_hours\n",
        "                    new_date2 = new_date1\n",
        "            current_date = add_interval(current_date, interval_type)\n",
        "        return {\n",
        "            'initial_entities': initial_entities,\n",
        "            'unit_arrival_pattern': unit_arrival_pattern,\n",
        "            'total_adjusted_hours': total_adjusted_hours\n",
        "        }\n",
        "\n",
        "    def processor_dependency_graphs(self):\n",
        "        \"\"\"\n",
        "        Visualize processor dependency graphs by p-level.\n",
        "        \"\"\"\n",
        "        if not self.results:\n",
        "            logger.warning(\"No results available. Run the simulation first.\")\n",
        "            return\n",
        "        last_p_level_nodes = set()\n",
        "        undirected_graph = nx.Graph()\n",
        "        p_levels = set()\n",
        "        for path_name, path_data in self.paths_info.items():\n",
        "            processing_times = path_data['processing_times']\n",
        "            G = path_data['graph']\n",
        "            for node in G.nodes():\n",
        "                if node in processing_times:\n",
        "                    p_times = processing_times[node].get('processing_time', {})\n",
        "                    if isinstance(p_times, dict):\n",
        "                        p_levels.update(p_times.keys())\n",
        "                    else:\n",
        "                        p_levels.update([1, 2])\n",
        "                    undirected_graph.add_node(node)\n",
        "                    for succ in G.successors(node):\n",
        "                        undirected_graph.add_edge(node, succ)\n",
        "                    p_level_succs = processing_times[node].get('p_level_successors', {})\n",
        "                    for succ_list in p_level_succs.values():\n",
        "                        for succ in succ_list:\n",
        "                            undirected_graph.add_edge(node, succ)\n",
        "        print(\"Plotting the undirected graph with all nodes and edges across P-levels.\")\n",
        "        pos = nx.spring_layout(undirected_graph)\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        nx.draw(undirected_graph, pos, with_labels=True, node_color='lightblue', edge_color='gray',\n",
        "                node_size=1500, font_size=10)\n",
        "        plt.title('Undirected Processor Dependency Graph (All P-levels)')\n",
        "        plt.show()\n",
        "        sorted_p_levels = sorted(p_levels)\n",
        "        for idx, p_level in enumerate(sorted_p_levels):\n",
        "            print(f\"Plotting subgraph for p-level {p_level}\")\n",
        "            p_level_graph = nx.DiGraph()\n",
        "            for path_name, path_data in self.paths_info.items():\n",
        "                processing_times = path_data['processing_times']\n",
        "                G = path_data['graph']\n",
        "                for node in processing_times:\n",
        "                    p_times = processing_times[node].get('processing_time', {})\n",
        "                    if isinstance(p_times, dict):\n",
        "                        if p_level in p_times:\n",
        "                            p_level_graph.add_node(node)\n",
        "                    else:\n",
        "                        p_level_graph.add_node(node)\n",
        "                for node in p_level_graph.nodes():\n",
        "                    if node in processing_times:\n",
        "                        p_level_succs = processing_times[node].get('p_level_successors', {})\n",
        "                        successors = p_level_succs.get(p_level, None)\n",
        "                        if successors is not None:\n",
        "                            for succ in successors:\n",
        "                                if succ in p_level_graph.nodes():\n",
        "                                    p_level_graph.add_edge(node, succ)\n",
        "                        else:\n",
        "                            for succ in G.successors(node):\n",
        "                                if succ in p_level_graph.nodes():\n",
        "                                    p_level_graph.add_edge(node, succ)\n",
        "            current_p_level_nodes = set(p_level_graph.nodes())\n",
        "            new_nodes = current_p_level_nodes - last_p_level_nodes\n",
        "            removed_nodes = last_p_level_nodes - current_p_level_nodes\n",
        "            remaining_nodes = current_p_level_nodes & last_p_level_nodes\n",
        "            last_p_level_nodes = current_p_level_nodes\n",
        "            node_colors = []\n",
        "            for node in p_level_graph.nodes():\n",
        "                if node in self.p_level_changers:\n",
        "                    color = 'orange'\n",
        "                elif node in self.path_changers:\n",
        "                    color = 'green'\n",
        "                elif node in self.priority_changers:\n",
        "                    color = 'red'\n",
        "                elif node in new_nodes:\n",
        "                    color = 'lightgreen'\n",
        "                elif node in remaining_nodes:\n",
        "                    color = 'lightblue'\n",
        "                else:\n",
        "                    color = 'gray'\n",
        "                node_colors.append(color)\n",
        "            plt.figure(figsize=(12, 10))\n",
        "            pos = nx.spring_layout(p_level_graph)\n",
        "            nx.draw(p_level_graph, pos, with_labels=True, node_color=node_colors,\n",
        "                    edge_color='black', node_size=1500, font_size=10)\n",
        "            from matplotlib.patches import Patch\n",
        "            legend_elements = [\n",
        "                Patch(facecolor='lightgreen', edgecolor='black', label='New Node'),\n",
        "                Patch(facecolor='lightblue', edgecolor='black', label='Existing Node'),\n",
        "                Patch(facecolor='orange', edgecolor='black', label='P-Level Changer'),\n",
        "                Patch(facecolor='green', edgecolor='black', label='Path Changer'),\n",
        "                Patch(facecolor='red', edgecolor='black', label='Priority Changer')\n",
        "            ]\n",
        "            plt.legend(handles=legend_elements, loc='best')\n",
        "            plt.title(f'Processor Dependency Graph for P-Level {p_level}')\n",
        "            plt.show()\n",
        "\n",
        "    def balance_parameters(self, param_map, k_max, error_constraint='either', excluded_processors=None):\n",
        "        \"\"\"\n",
        "        Balances parameters into sets of sizes from 2 to k_max.\n",
        "\n",
        "        Parameters:\n",
        "        - param_map: Dictionary mapping parameter names to values\n",
        "        - k_max: Maximum size of the sets\n",
        "        - error_constraint: 'positive', 'negative', or 'either'\n",
        "        - excluded_processors: List of processor names to exclude\n",
        "\n",
        "        Returns:\n",
        "        - Dictionary where keys are sizes from 2 to k_max, and values are lists of sets\n",
        "        \"\"\"\n",
        "        if excluded_processors is None:\n",
        "            excluded_processors = []\n",
        "        parameters = list(param_map.items())\n",
        "        if excluded_processors:\n",
        "            parameters = [(name, value) for name, value in parameters if name not in excluded_processors]\n",
        "        all_params = set(name for name, value in parameters)\n",
        "        total_params = len(all_params)\n",
        "        results = {}\n",
        "        def get_combinations(parameters, k, error_constraint='either'):\n",
        "            combos = list(combinations(parameters, k))\n",
        "            combo_sums = []\n",
        "            for combo in combos:\n",
        "                names = [name for name, value in combo]\n",
        "                total = sum(value for name, value in combo)\n",
        "                if error_constraint == 'positive' and total < 0:\n",
        "                    continue\n",
        "                if error_constraint == 'negative' and total > 0:\n",
        "                    continue\n",
        "                combo_sums.append({'names': names, 'total': total, 'abs_total': abs(total)})\n",
        "            combo_sums.sort(key=lambda x: x['abs_total'])\n",
        "            return combo_sums\n",
        "        def select_combinations(combo_sums):\n",
        "            selected_sets = []\n",
        "            used_params = set()\n",
        "            remaining_params = set(all_params)\n",
        "            while remaining_params:\n",
        "                for combo in combo_sums:\n",
        "                    names = combo['names']\n",
        "                    new_params = set(names) & remaining_params\n",
        "                    combo['new_params'] = len(new_params)\n",
        "                combo_sums.sort(key=lambda x: (-x['new_params'], x['abs_total']))\n",
        "                for combo in combo_sums:\n",
        "                    if combo['new_params'] > 0:\n",
        "                        selected_sets.append(combo)\n",
        "                        used_params.update(combo['names'])\n",
        "                        remaining_params -= set(combo['names'])\n",
        "                        break\n",
        "                else:\n",
        "                    if not combo_sums:\n",
        "                        break\n",
        "                    combo_sums.sort(key=lambda x: x['abs_total'])\n",
        "                    selected_sets.append(combo_sums[0])\n",
        "                    used_params.update(combo_sums[0]['names'])\n",
        "                    remaining_params -= set(combo_sums[0]['names'])\n",
        "                    break\n",
        "            return selected_sets\n",
        "        for k in range(2, k_max + 1):\n",
        "            combo_sums = get_combinations(parameters, k, error_constraint=error_constraint)\n",
        "            selected_sets = select_combinations(combo_sums)\n",
        "            sum_positives = sum(combo['total'] for combo in selected_sets if combo['total'] > 0)\n",
        "            sum_negatives = sum(combo['total'] for combo in selected_sets if combo['total'] < 0)\n",
        "            results[k] = {\n",
        "                'sets': selected_sets,\n",
        "                'sum_positives': sum_positives,\n",
        "                'sum_negatives': sum_negatives\n",
        "            }\n",
        "        return results\n",
        "\n",
        "    def balance_parameters_dp(self, param_map, k_max, error_constraint='either', excluded_processors=None):\n",
        "        \"\"\"\n",
        "        Balances parameters into sets of sizes from 2 to k_max using dynamic programming.\n",
        "\n",
        "        Parameters:\n",
        "        - param_map: Dictionary mapping parameter names to values\n",
        "        - k_max: Maximum size of the sets\n",
        "        - error_constraint: 'positive', 'negative', or 'either'\n",
        "        - excluded_processors: List of processor names to exclude\n",
        "\n",
        "        Returns:\n",
        "        - Dictionary where keys are sizes from 2 to k_max, and values are lists of sets\n",
        "        \"\"\"\n",
        "        if excluded_processors is None:\n",
        "            excluded_processors = []\n",
        "        if excluded_processors:\n",
        "            param_map = {name: value for name, value in param_map.items() if name not in excluded_processors}\n",
        "        parameters = list(param_map.items())\n",
        "        total_params = len(parameters)\n",
        "        results = {}\n",
        "        for k in range(2, k_max + 1):\n",
        "            all_combos = list(combinations(parameters, k))\n",
        "            dp = {}\n",
        "            for combo in all_combos:\n",
        "                names = tuple(name for name, value in combo)\n",
        "                total = sum(value for name, value in combo)\n",
        "                if error_constraint == 'positive' and total < 0:\n",
        "                    continue\n",
        "                if error_constraint == 'negative' and total > 0:\n",
        "                    continue\n",
        "                abs_total = abs(total)\n",
        "                dp[names] = abs_total\n",
        "            sorted_combos = sorted(dp.items(), key=lambda x: x[1])\n",
        "            selected_sets = []\n",
        "            used_params = set()\n",
        "            remaining_params = set(name for name, value in parameters)\n",
        "            for names, abs_total in sorted_combos:\n",
        "                names_set = set(names)\n",
        "                if names_set & remaining_params:\n",
        "                    selected_sets.append({\n",
        "                        'names': names,\n",
        "                        'total': sum(param_map[name] for name in names)\n",
        "                    })\n",
        "                    used_params.update(names)\n",
        "                    remaining_params -= names_set\n",
        "                    if not remaining_params:\n",
        "                        break\n",
        "            if remaining_params:\n",
        "                for names, abs_total in sorted_combos:\n",
        "                    names_set = set(names)\n",
        "                    if names_set & remaining_params:\n",
        "                        selected_sets.append({\n",
        "                            'names': names,\n",
        "                            'total': sum(param_map[name] for name in names)\n",
        "                        })\n",
        "                        used_params.update(names)\n",
        "                        remaining_params -= names_set\n",
        "                        if not remaining_params:\n",
        "                            break\n",
        "            sum_positives = sum(combo['total'] for combo in selected_sets if combo['total'] > 0)\n",
        "            sum_negatives = sum(combo['total'] for combo in selected_sets if combo['total'] < 0)\n",
        "            results[k] = {\n",
        "                'sets': selected_sets,\n",
        "                'sum_positives': sum_positives,\n",
        "                'sum_negatives': sum_negatives\n",
        "            }\n",
        "        return results\n",
        "\n",
        "def system_Optimization_model(\n",
        "    params,\n",
        "    mDaysShiftHours,\n",
        "    wc24_3rdShiftAdj,\n",
        "    n_entities,\n",
        "    start_date,\n",
        "    interval_type,\n",
        "    end_date,\n",
        "    initial_num_entities,\n",
        "    arrival_pattern_num_entities,\n",
        "    remove_weekends,\n",
        "    holiday_dates,\n",
        "    subtract_pto_hours_per_week,\n",
        "    scale_days,\n",
        "    remove_holiday_overlap,\n",
        "    paths,\n",
        "    p_level_changers,\n",
        "    path_changers,\n",
        "    priority_changers,\n",
        "    exclude_processors,\n",
        "    top_n,\n",
        "    max_processors\n",
        "):\n",
        "    \"\"\"\n",
        "    Simulating processing and calculating idle times and dependencies using SimPy.\n",
        "\n",
        "    Parameters:\n",
        "    - params: List of adjustment parameters for processors\n",
        "    - Various simulation configuration parameters\n",
        "\n",
        "    Returns:\n",
        "    - total_idle_time, total_processing_time, total_adjusted_hours\n",
        "    \"\"\"\n",
        "    a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, a11, a12, a13, a14, a15 = params\n",
        "    sim = ProcessSimulation([], [], {}, {}, {}, {}, [], 0)\n",
        "    arrival_results = sim.create_entity_arrival_schedule(\n",
        "        start_date=start_date,\n",
        "        interval_type=interval_type,\n",
        "        end_date=end_date,\n",
        "        initial_num_entities=initial_num_entities,\n",
        "        arrival_pattern_num_entities=arrival_pattern_num_entities,\n",
        "        remove_weekends=remove_weekends,\n",
        "        holiday_dates=holiday_dates,\n",
        "        subtract_pto_hours_per_week=subtract_pto_hours_per_week,\n",
        "        scale_days=mDaysShiftHours,\n",
        "        remove_holiday_overlap=remove_holiday_overlap\n",
        "    )\n",
        "    initial_entities = arrival_results['initial_entities']\n",
        "    unit_arrival_pattern = arrival_results['unit_arrival_pattern']\n",
        "    total_adjusted_hours = arrival_results['total_adjusted_hours']\n",
        "    scalerWc05 = ((mDaysShiftHours + a1) / mDaysShiftHours)\n",
        "    scalerWc03 = ((mDaysShiftHours + a2) / mDaysShiftHours)\n",
        "    scalerWc21 = ((mDaysShiftHours + a3) / mDaysShiftHours)\n",
        "    scalerWc02 = ((mDaysShiftHours + a4) / mDaysShiftHours)\n",
        "    scalerWc08 = ((mDaysShiftHours + a5) / mDaysShiftHours)\n",
        "    scalerWc07 = ((mDaysShiftHours + a6) / mDaysShiftHours)\n",
        "    scalerWc06 = ((mDaysShiftHours + a7) / mDaysShiftHours)\n",
        "    scalerWc22 = ((mDaysShiftHours + a8) / mDaysShiftHours)\n",
        "    scalerWc23 = ((mDaysShiftHours + a9) / mDaysShiftHours)\n",
        "    scalerWc11 = ((mDaysShiftHours + a10) / mDaysShiftHours)\n",
        "    scalerWc19 = ((mDaysShiftHours + a11) / mDaysShiftHours)\n",
        "    scalerWc15 = ((mDaysShiftHours + a12) / mDaysShiftHours)\n",
        "    scalerWc17 = ((mDaysShiftHours + a13) / mDaysShiftHours)\n",
        "    scalerWc24 = ((mDaysShiftHours + a14) / mDaysShiftHours)\n",
        "    scalerWc20 = ((mDaysShiftHours + a15) / mDaysShiftHours)\n",
        "    paths_info = {\n",
        "        'Path1': {\n",
        "            'processing_times': {\n",
        "                'Start_Job': {'processing_time': 0, 'capacity': 1},\n",
        "                'WC05': {'processing_time': 39/scalerWc05, 'capacity': 1},\n",
        "                'WC03': {'processing_time': 39/scalerWc03, 'capacity': 1},\n",
        "                'WC21': {'processing_time': 32/scalerWc21, 'capacity': 1},\n",
        "                'WC02': {'processing_time': 32/scalerWc02, 'capacity': 1},\n",
        "                'WC06': {'processing_time': 6/scalerWc06, 'capacity': 1},\n",
        "                'WC24': {\n",
        "                    'processing_time': {\n",
        "                        1: 19/scalerWc24,\n",
        "                        2: (((24-wc24_3rdShiftAdj)/scalerWc24) + (24-wc24_3rdShiftAdj) + (24-wc24_3rdShiftAdj))\n",
        "                    },\n",
        "                    'capacity': 2,\n",
        "                    'p_level_successors': {\n",
        "                        1: ['WC11'],\n",
        "                        2: ['WC17']\n",
        "                    }\n",
        "                },\n",
        "                'WC08': {'processing_time': 6/scalerWc08, 'capacity': 1},\n",
        "                'WC07': {'processing_time': 32/scalerWc07, 'capacity': 1},\n",
        "                'WC11': {'processing_time': 16/scalerWc11, 'capacity': 1},\n",
        "                'WC19': {'processing_time': 32/scalerWc19, 'capacity': 1},\n",
        "                'WC22': {'processing_time': 13/scalerWc22, 'capacity': 1},\n",
        "                'WC23': {'processing_time': 32/scalerWc23, 'capacity': 1},\n",
        "                'WC15': {'processing_time': 54/scalerWc15, 'capacity': 1},\n",
        "                'WC17': {\n",
        "                    'processing_time': {\n",
        "                        1: 35/scalerWc17,\n",
        "                        2: 13/scalerWc17\n",
        "                    },\n",
        "                    'capacity': 2,\n",
        "                    'p_level_successors': {\n",
        "                        1: ['WC20'],\n",
        "                        2: ['Units_Delivered']\n",
        "                    }\n",
        "                },\n",
        "                'WC20': {\n",
        "                    'processing_time': 45/scalerWc20,\n",
        "                    'capacity': 1,\n",
        "                    'p_level_successors': {2: ['WC24']}\n",
        "                },\n",
        "                'Units_Delivered': {'processing_time': 0, 'capacity': 1}\n",
        "            },\n",
        "            'graph': nx.DiGraph([\n",
        "                ('Start_Job', 'WC05'), ('Start_Job', 'WC02'), ('Start_Job', 'WC08'),\n",
        "                ('Start_Job', 'WC22'), ('Start_Job', 'WC23'), ('WC05', 'WC03'),\n",
        "                ('WC03', 'WC21'), ('WC21', 'WC06'), ('WC02', 'WC06'),\n",
        "                ('WC06', 'WC24'), ('WC24', 'WC11'), ('WC24', 'WC17'),\n",
        "                ('WC08', 'WC07'), ('WC07', 'WC11'), ('WC22', 'WC11'),\n",
        "                ('WC11', 'WC19'), ('WC19', 'WC15'), ('WC23', 'WC15'),\n",
        "                ('WC15', 'WC17'), ('WC17', 'WC20'), ('WC17', 'Units_Delivered')\n",
        "            ])\n",
        "        }\n",
        "    }\n",
        "    simulation = ProcessSimulation(\n",
        "        initial_entities=initial_entities,\n",
        "        unit_arrival_pattern=unit_arrival_pattern,\n",
        "        paths_info=paths_info,\n",
        "        p_level_changers=p_level_changers,\n",
        "        path_changers=path_changers,\n",
        "        priority_changers=priority_changers,\n",
        "        exclude_processors=exclude_processors,\n",
        "        n_entities=n_entities,\n",
        "        top_n=top_n\n",
        "    )\n",
        "    results = simulation.run()\n",
        "    total_idle_time = sum(results['idle_times'].values())\n",
        "    total_processing_time = results['total_processing_time']\n",
        "    return total_idle_time, total_processing_time, total_adjusted_hours\n",
        "\n",
        "def optimize_parameters(\n",
        "    initial_params,\n",
        "    bounds,\n",
        "    fixed_params,\n",
        "    increments,\n",
        "    integer_indices,\n",
        "    total_adjusted_hours,\n",
        "    mDaysShiftHours,\n",
        "    wc24_3rdShiftAdj,\n",
        "    n_entities,\n",
        "    start_date,\n",
        "    interval_type,\n",
        "    end_date,\n",
        "    initial_num_entities,\n",
        "    arrival_pattern_num_entities,\n",
        "    remove_weekends,\n",
        "    holiday_dates,\n",
        "    subtract_pto_hours_per_week,\n",
        "    scale_days,\n",
        "    remove_holiday_overlap,\n",
        "    paths,\n",
        "    p_level_changers,\n",
        "    path_changers,\n",
        "    priority_changers,\n",
        "    exclude_processors,\n",
        "    top_n,\n",
        "    max_processors\n",
        "):\n",
        "    \"\"\"\n",
        "    Optimize parameters for the simulation model.\n",
        "\n",
        "    Parameters:\n",
        "    - initial_params: Starting parameter values\n",
        "    - bounds: Min/max bounds for each parameter\n",
        "    - fixed_params: Parameters that should not be optimized\n",
        "    - And various other simulation configuration parameters\n",
        "\n",
        "    Returns:\n",
        "    - Optimized parameter values\n",
        "    \"\"\"\n",
        "    from scipy.optimize import minimize\n",
        "    def objective(params):\n",
        "        params = np.array(params)\n",
        "        for idx, val in fixed_params.items():\n",
        "            params[idx] = val\n",
        "        for idx in integer_indices:\n",
        "            increment = increments.get(idx, 1)\n",
        "            params[idx] = np.round(params[idx] / increment) * increment\n",
        "        total_idle_time, _, _ = system_Optimization_model(\n",
        "            params,\n",
        "            mDaysShiftHours,\n",
        "            wc24_3rdShiftAdj,\n",
        "            n_entities,\n",
        "            start_date,\n",
        "            interval_type,\n",
        "            end_date,\n",
        "            initial_num_entities,\n",
        "            arrival_pattern_num_entities,\n",
        "            remove_weekends,\n",
        "            holiday_dates,\n",
        "            subtract_pto_hours_per_week,\n",
        "            scale_days,\n",
        "            remove_holiday_overlap,\n",
        "            paths,\n",
        "            p_level_changers,\n",
        "            path_changers,\n",
        "            priority_changers,\n",
        "            exclude_processors,\n",
        "            top_n,\n",
        "            max_processors\n",
        "        )\n",
        "        return -total_idle_time\n",
        "    def constraint_total_time(params):\n",
        "        params = np.array(params)\n",
        "        for idx, val in fixed_params.items():\n",
        "            params[idx] = val\n",
        "        for idx in integer_indices:\n",
        "            increment = increments.get(idx, 1)\n",
        "            params[idx] = np.round(params[idx] / increment) * increment\n",
        "        _, total_processing_time, _ = system_Optimization_model(\n",
        "            params,\n",
        "            mDaysShiftHours,\n",
        "            wc24_3rdShiftAdj,\n",
        "            n_entities,\n",
        "            start_date,\n",
        "            interval_type,\n",
        "            end_date,\n",
        "            initial_num_entities,\n",
        "            arrival_pattern_num_entities,\n",
        "            remove_weekends,\n",
        "            holiday_dates,\n",
        "            subtract_pto_hours_per_week,\n",
        "            scale_days,\n",
        "            remove_holiday_overlap,\n",
        "            paths,\n",
        "            p_level_changers,\n",
        "            path_changers,\n",
        "            priority_changers,\n",
        "            exclude_processors,\n",
        "            top_n,\n",
        "            max_processors\n",
        "        )\n",
        "        return total_adjusted_hours - total_processing_time\n",
        "    constraints = [{'type': 'ineq', 'fun': constraint_total_time}]\n",
        "    adjusted_bounds = []\n",
        "    for i, (lower, upper) in enumerate(bounds):\n",
        "        if i in fixed_params:\n",
        "            adjusted_bounds.append((fixed_params[i], fixed_params[i]))\n",
        "        else:\n",
        "            adjusted_bounds.append((lower, upper))\n",
        "    result = minimize(\n",
        "        objective,\n",
        "        initial_params,\n",
        "        bounds=adjusted_bounds,\n",
        "        constraints=constraints,\n",
        "        method='SLSQP',\n",
        "        options={'maxiter': 100}\n",
        "    )\n",
        "    optimized_params = result.x\n",
        "    for idx, val in fixed_params.items():\n",
        "        optimized_params[idx] = val\n",
        "    for idx in integer_indices:\n",
        "        increment = increments.get(idx, 1)\n",
        "        optimized_params[idx] = np.round(optimized_params[idx] / increment) * increment\n",
        "    return optimized_params\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the SimPy simulation.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "    n_entities = 189\n",
        "    mDaysShiftHours = 6.5\n",
        "    wc24_3rdShiftAdj = 11\n",
        "    start_date = \"2025-01-01\"\n",
        "    interval_type = \"monthly\"\n",
        "    end_date = \"2029-05-25\"\n",
        "    initial_num_entities = 1\n",
        "    arrival_pattern_num_entities = 4\n",
        "    remove_weekends = True\n",
        "    holiday_dates = [\n",
        "        \"2025-01-01\", \"2025-05-25\", \"2025-07-04\", \"2025-09-01\", \"2025-11-23\",\n",
        "        \"2025-11-24\", \"2025-12-24\", \"2025-12-25\", \"2025-12-26\", \"2025-12-27\",\n",
        "        \"2025-12-28\", \"2025-12-29\", \"2026-01-01\", \"2026-05-25\", \"2026-07-04\",\n",
        "    ]\n",
        "    subtract_pto_hours_per_week = 3.5 * 0.9\n",
        "    remove_holiday_overlap = True\n",
        "    exclude_processors = ['Start_Job', 'Units_Delivered']\n",
        "    paths = ['Path1']\n",
        "    sim = ProcessSimulation([], [], {}, n_entities=0)\n",
        "    arrival_results = sim.create_entity_arrival_schedule(\n",
        "        start_date=start_date,\n",
        "        interval_type=interval_type,\n",
        "        end_date=end_date,\n",
        "        initial_num_entities=initial_num_entities,\n",
        "        arrival_pattern_num_entities=arrival_pattern_num_entities,\n",
        "        remove_weekends=remove_weekends,\n",
        "        holiday_dates=holiday_dates,\n",
        "        subtract_pto_hours_per_week=subtract_pto_hours_per_week,\n",
        "        scale_days=mDaysShiftHours,\n",
        "        remove_holiday_overlap=remove_holiday_overlap\n",
        "    )\n",
        "    initial_entities = arrival_results['initial_entities']\n",
        "    unit_arrival_pattern = arrival_results['unit_arrival_pattern']\n",
        "    total_adjusted_hours = arrival_results['total_adjusted_hours']\n",
        "    print(\"\\nTotal adjusted hours:\", total_adjusted_hours)\n",
        "    adjusted_hours_per_entity = total_adjusted_hours / n_entities\n",
        "    print(f\"Adjusted hours per entity: {adjusted_hours_per_entity:.2f}\")\n",
        "    a1 = 0\n",
        "    a2 = 0\n",
        "    a3 = 0\n",
        "    a4 = 0\n",
        "    a5 = 0\n",
        "    a6 = 0\n",
        "    a7 = 0\n",
        "    a8 = 0\n",
        "    a9 = 0\n",
        "    a10 = 0\n",
        "    a11 = 0\n",
        "    a12 = 0\n",
        "    a13 = 0\n",
        "    a14 = 0\n",
        "    a15 = 0\n",
        "    scalerWc05 = (mDaysShiftHours + a1) / mDaysShiftHours\n",
        "    scalerWc03 = (mDaysShiftHours + a2) / mDaysShiftHours\n",
        "    scalerWc21 = (mDaysShiftHours + a3) / mDaysShiftHours\n",
        "    scalerWc02 = (mDaysShiftHours + a4) / mDaysShiftHours\n",
        "    scalerWc08 = (mDaysShiftHours + a5) / mDaysShiftHours\n",
        "    scalerWc07 = (mDaysShiftHours + a6) / mDaysShiftHours\n",
        "    scalerWc06 = (mDaysShiftHours + a7) / mDaysShiftHours\n",
        "    scalerWc22 = (mDaysShiftHours + a8) / mDaysShiftHours\n",
        "    scalerWc23 = (mDaysShiftHours + a9) / mDaysShiftHours\n",
        "    scalerWc11 = (mDaysShiftHours + a10) / mDaysShiftHours\n",
        "    scalerWc19 = (mDaysShiftHours + a11) / mDaysShiftHours\n",
        "    scalerWc15 = (mDaysShiftHours + a12) / mDaysShiftHours\n",
        "    scalerWc17 = (mDaysShiftHours + a13) / mDaysShiftHours\n",
        "    scalerWc24 = (mDaysShiftHours + a14) / mDaysShiftHours\n",
        "    scalerWc20 = (mDaysShiftHours + a15) / mDaysShiftHours\n",
        "    p_level_changers = {\n",
        "        'WC20': lambda p: 2 if p == 1 else 1,\n",
        "        'WC24': lambda p: p,\n",
        "        'WC17': lambda p: p\n",
        "    }\n",
        "    path_changers = {}\n",
        "    priority_changers = {\n",
        "        'WC20': lambda priority: 0,\n",
        "        'WC24': lambda priority: priority,\n",
        "        'WC17': lambda priority: priority\n",
        "    }\n",
        "    paths_info = {\n",
        "        'Path1': {\n",
        "            'processing_times': {\n",
        "                'Start_Job': {'processing_time': 0, 'capacity': 1},\n",
        "                'WC05': {'processing_time': 39/scalerWc05, 'capacity': 1},\n",
        "                'WC03': {'processing_time': 39/scalerWc03, 'capacity': 1},\n",
        "                'WC21': {'processing_time': 32/scalerWc21, 'capacity': 1},\n",
        "                'WC02': {'processing_time': 32/scalerWc02, 'capacity': 1},\n",
        "                'WC06': {'processing_time': 6/scalerWc06, 'capacity': 1},\n",
        "                'WC24': {\n",
        "                    'processing_time': {\n",
        "                        1: 19/scalerWc24,\n",
        "                        2: (((24-wc24_3rdShiftAdj)/scalerWc24) + (24-wc24_3rdShiftAdj) + (24-wc24_3rdShiftAdj))\n",
        "                    },\n",
        "                    'capacity': 2,\n",
        "                    'p_level_successors': {\n",
        "                        1: ['WC11'],\n",
        "                        2: ['WC17']\n",
        "                    }\n",
        "                },\n",
        "                'WC08': {'processing_time': 6/scalerWc08, 'capacity': 1},\n",
        "                'WC07': {'processing_time': 32/scalerWc07, 'capacity': 1},\n",
        "                'WC11': {'processing_time': 16/scalerWc11, 'capacity': 1},\n",
        "                'WC19': {'processing_time': 32/scalerWc19, 'capacity': 1},\n",
        "                'WC22': {'processing_time': 13/scalerWc22, 'capacity': 1},\n",
        "                'WC23': {'processing_time': 32/scalerWc23, 'capacity': 1},\n",
        "                'WC15': {'processing_time': 54/scalerWc15, 'capacity': 1},\n",
        "                'WC17': {\n",
        "                    'processing_time': {\n",
        "                        1: 35/scalerWc17,\n",
        "                        2: 13/scalerWc17\n",
        "                    },\n",
        "                    'capacity': 2,\n",
        "                    'p_level_successors': {\n",
        "                        1: ['WC20'],\n",
        "                        2: ['Units_Delivered']\n",
        "                    }\n",
        "                },\n",
        "                'WC20': {\n",
        "                    'processing_time': 45/scalerWc20,\n",
        "                    'capacity': 1,\n",
        "                    'p_level_successors': {2: ['WC24']}\n",
        "                },\n",
        "                'Units_Delivered': {'processing_time': 0, 'capacity': 1}\n",
        "            },\n",
        "            'graph': nx.DiGraph([\n",
        "                ('Start_Job', 'WC05'), ('Start_Job', 'WC02'), ('Start_Job', 'WC08'),\n",
        "                ('Start_Job', 'WC22'), ('Start_Job', 'WC23'), ('WC05', 'WC03'),\n",
        "                ('WC03', 'WC21'), ('WC21', 'WC06'), ('WC02', 'WC06'),\n",
        "                ('WC06', 'WC24'), ('WC24', 'WC11'), ('WC24', 'WC17'),\n",
        "                ('WC08', 'WC07'), ('WC07', 'WC11'), ('WC22', 'WC11'),\n",
        "                ('WC11', 'WC19'), ('WC19', 'WC15'), ('WC23', 'WC15'),\n",
        "                ('WC15', 'WC17'), ('WC17', 'WC20'), ('WC17', 'Units_Delivered')\n",
        "            ])\n",
        "        }\n",
        "    }\n",
        "    print(\"Running simulation with initial parameters...\")\n",
        "    simulation = ProcessSimulation(\n",
        "        initial_entities=initial_entities,\n",
        "        unit_arrival_pattern=unit_arrival_pattern,\n",
        "        paths_info=paths_info,\n",
        "        p_level_changers=p_level_changers,\n",
        "        path_changers=path_changers,\n",
        "        priority_changers=priority_changers,\n",
        "        exclude_processors=exclude_processors,\n",
        "        n_entities=n_entities\n",
        "    )\n",
        "    results = simulation.run()\n",
        "    simulation.plot_processing_schedule()\n",
        "    simulation.plot_results()\n",
        "    simulation.plot_results_individual_processors()\n",
        "    simulation.processor_dependency_graphs()\n",
        "    initial_params = [2, 2, 0.5, 0, -5, 0.5, -5, -3.5, 0.5, -3.0, 0.5, 5, -1, 0, 3.5]\n",
        "    bounds = [(-6.4, 6.5)] * 15\n",
        "    fixed_params = {3: 0, 13: 0}\n",
        "    increments = {}\n",
        "    integer_indices = []\n",
        "    print(\"\\nOptimizing parameters...\")\n",
        "    optimized_params = optimize_parameters(\n",
        "        initial_params,\n",
        "        bounds,\n",
        "        fixed_params,\n",
        "        increments,\n",
        "        integer_indices,\n",
        "        total_adjusted_hours,\n",
        "        mDaysShiftHours,\n",
        "        wc24_3rdShiftAdj,\n",
        "        n_entities,\n",
        "        start_date,\n",
        "        interval_type,\n",
        "        end_date,\n",
        "        initial_num_entities,\n",
        "        arrival_pattern_num_entities,\n",
        "        remove_weekends,\n",
        "        holiday_dates,\n",
        "        subtract_pto_hours_per_week,\n",
        "        mDaysShiftHours,\n",
        "        remove_holiday_overlap,\n",
        "        paths,\n",
        "        p_level_changers,\n",
        "        path_changers,\n",
        "        priority_changers,\n",
        "        exclude_processors,\n",
        "        0,\n",
        "        1\n",
        "    )\n",
        "    print(\"\\nOptimized Parameters:\")\n",
        "    optimized_param_map = {}\n",
        "    for i, val in enumerate(optimized_params):\n",
        "        param_name = f\"a{i+1}\"\n",
        "        print(f\"{param_name} = {val}\")\n",
        "        if i == 0:\n",
        "            optimized_param_map['WC05'] = val\n",
        "        elif i == 1:\n",
        "            optimized_param_map['WC03'] = val\n",
        "        elif i == 2:\n",
        "            optimized_param_map['WC21'] = val\n",
        "        elif i == 3:\n",
        "            optimized_param_map['WC02'] = val\n",
        "        elif i == 4:\n",
        "            optimized_param_map['WC08'] = val\n",
        "        elif i == 5:\n",
        "            optimized_param_map['WC07'] = val\n",
        "        elif i == 6:\n",
        "            optimized_param_map['WC06'] = val\n",
        "        elif i == 7:\n",
        "            optimized_param_map['WC22'] = val\n",
        "        elif i == 8:\n",
        "            optimized_param_map['WC23'] = val\n",
        "        elif i == 9:\n",
        "            optimized_param_map['WC11'] = val\n",
        "        elif i == 10:\n",
        "            optimized_param_map['WC19'] = val\n",
        "        elif i == 11:\n",
        "            optimized_param_map['WC15'] = val\n",
        "        elif i == 12:\n",
        "            optimized_param_map['WC17'] = val\n",
        "        elif i == 13:\n",
        "            optimized_param_map['WC24'] = val\n",
        "        elif i == 14:\n",
        "            optimized_param_map['WC20'] = val\n",
        "    print(\"\\nRunning simulation with optimized parameters...\")\n",
        "    a1 = optimized_params[0]\n",
        "    a2 = optimized_params[1]\n",
        "    a3 = optimized_params[2]\n",
        "    a4 = optimized_params[3]\n",
        "    a5 = optimized_params[4]\n",
        "    a6 = optimized_params[5]\n",
        "    a7 = optimized_params[6]\n",
        "    a8 = optimized_params[7]\n",
        "    a9 = optimized_params[8]\n",
        "    a10 = optimized_params[9]\n",
        "    a11 = optimized_params[10]\n",
        "    a12 = optimized_params[11]\n",
        "    a13 = optimized_params[12]\n",
        "    a14 = optimized_params[13]\n",
        "    a15 = optimized_params[14]\n",
        "    scalerWc05 = (mDaysShiftHours + a1) / mDaysShiftHours\n",
        "    scalerWc03 = (mDaysShiftHours + a2) / mDaysShiftHours\n",
        "    scalerWc21 = (mDaysShiftHours + a3) / mDaysShiftHours\n",
        "    scalerWc02 = (mDaysShiftHours + a4) / mDaysShiftHours\n",
        "    scalerWc08 = (mDaysShiftHours + a5) / mDaysShiftHours\n",
        "    scalerWc07 = (mDaysShiftHours + a6) / mDaysShiftHours\n",
        "    scalerWc06 = (mDaysShiftHours + a7) / mDaysShiftHours\n",
        "    scalerWc22 = (mDaysShiftHours + a8) / mDaysShiftHours\n",
        "    scalerWc23 = (mDaysShiftHours + a9) / mDaysShiftHours\n",
        "    scalerWc11 = (mDaysShiftHours + a10) / mDaysShiftHours\n",
        "    scalerWc19 = (mDaysShiftHours + a11) / mDaysShiftHours\n",
        "    scalerWc15 = (mDaysShiftHours + a12) / mDaysShiftHours\n",
        "    scalerWc17 = (mDaysShiftHours + a13) / mDaysShiftHours\n",
        "    scalerWc24 = (mDaysShiftHours + a14) / mDaysShiftHours\n",
        "    scalerWc20 = (mDaysShiftHours + a15) / mDaysShiftHours\n",
        "    optimized_paths_info = {\n",
        "        'Path1': {\n",
        "            'processing_times': {\n",
        "                'Start_Job': {'processing_time': 0, 'capacity': 1},\n",
        "                'WC05': {'processing_time': 39/scalerWc05, 'capacity': 1},\n",
        "                'WC03': {'processing_time': 39/scalerWc03, 'capacity': 1},\n",
        "                'WC21': {'processing_time': 32/scalerWc21, 'capacity': 1},\n",
        "                'WC02': {'processing_time': 32/scalerWc02, 'capacity': 1},\n",
        "                'WC06': {'processing_time': 6/scalerWc06, 'capacity': 1},\n",
        "                'WC24': {\n",
        "                    'processing_time': {\n",
        "                        1: 19/scalerWc24,\n",
        "                        2: (((24-wc24_3rdShiftAdj)/scalerWc24) + (24-wc24_3rdShiftAdj) + (24-wc24_3rdShiftAdj))\n",
        "                    },\n",
        "                    'capacity': 2,\n",
        "                    'p_level_successors': {\n",
        "                        1: ['WC11'],\n",
        "                        2: ['WC17']\n",
        "                    }\n",
        "                },\n",
        "                'WC08': {'processing_time': 6/scalerWc08, 'capacity': 1},\n",
        "                'WC07': {'processing_time': 32/scalerWc07, 'capacity': 1},\n",
        "                'WC11': {'processing_time': 16/scalerWc11, 'capacity': 1},\n",
        "                'WC19': {'processing_time': 32/scalerWc19, 'capacity': 1},\n",
        "                'WC22': {'processing_time': 13/scalerWc22, 'capacity': 1},\n",
        "                'WC23': {'processing_time': 32/scalerWc23, 'capacity': 1},\n",
        "                'WC15': {'processing_time': 54/scalerWc15, 'capacity': 1},\n",
        "                'WC17': {\n",
        "                    'processing_time': {\n",
        "                        1: 35/scalerWc17,\n",
        "                        2: 13/scalerWc17\n",
        "                    },\n",
        "                    'capacity': 2,\n",
        "                    'p_level_successors': {\n",
        "                        1: ['WC20'],\n",
        "                        2: ['Units_Delivered']\n",
        "                    }\n",
        "                },\n",
        "                'WC20': {\n",
        "                    'processing_time': 45/scalerWc20,\n",
        "                    'capacity': 1,\n",
        "                    'p_level_successors': {2: ['WC24']}\n",
        "                },\n",
        "                'Units_Delivered': {'processing_time': 0, 'capacity': 1}\n",
        "            },\n",
        "            'graph': nx.DiGraph([\n",
        "                ('Start_Job', 'WC05'), ('Start_Job', 'WC02'), ('Start_Job', 'WC08'),\n",
        "                ('Start_Job', 'WC22'), ('Start_Job', 'WC23'), ('WC05', 'WC03'),\n",
        "                ('WC03', 'WC21'), ('WC21', 'WC06'), ('WC02', 'WC06'),\n",
        "                ('WC06', 'WC24'), ('WC24', 'WC11'), ('WC24', 'WC17'),\n",
        "                ('WC08', 'WC07'), ('WC07', 'WC11'), ('WC22', 'WC11'),\n",
        "                ('WC11', 'WC19'), ('WC19', 'WC15'), ('WC23', 'WC15'),\n",
        "                ('WC15', 'WC17'), ('WC17', 'WC20'), ('WC17', 'Units_Delivered')\n",
        "            ])\n",
        "        }\n",
        "    }\n",
        "    optimized_simulation = ProcessSimulation(\n",
        "        initial_entities=initial_entities,\n",
        "        unit_arrival_pattern=unit_arrival_pattern,\n",
        "        paths_info=optimized_paths_info,\n",
        "        p_level_changers=p_level_changers,\n",
        "        path_changers=path_changers,\n",
        "        priority_changers=priority_changers,\n",
        "        exclude_processors=exclude_processors,\n",
        "        n_entities=n_entities\n",
        "    )\n",
        "    optimized_results = optimized_simulation.run()\n",
        "    optimized_simulation.plot_processing_schedule()\n",
        "    optimized_simulation.plot_results()\n",
        "    optimized_simulation.plot_results_individual_processors()\n",
        "    print(\"\\nBalancing parameters...\")\n",
        "    excluded_processors_for_balance = ['WC02', 'WC24']\n",
        "    for error_constraint in ['either', 'negative']:\n",
        "        print(f\"\\nParameter balancing with {error_constraint} constraint:\")\n",
        "        print(\"\\nStandard parameter balancing:\")\n",
        "        results_bp = optimized_simulation.balance_parameters(\n",
        "            optimized_param_map,\n",
        "            k_max=15,\n",
        "            error_constraint=error_constraint,\n",
        "            excluded_processors=excluded_processors_for_balance\n",
        "        )\n",
        "        for k in results_bp:\n",
        "            print(f\"\\nSets of size {k}:\")\n",
        "            total_sets = results_bp[k]['sets']\n",
        "            for idx, combo in enumerate(total_sets):\n",
        "                names = combo['names']\n",
        "                total = combo['total']\n",
        "                print(f\"  Set {idx + 1}: {names}, Net Value: {total}\")\n",
        "            print(f\"  Sum of Positives: {results_bp[k]['sum_positives']}\")\n",
        "            print(f\"  Sum of Negatives: {results_bp[k]['sum_negatives']}\")\n",
        "        print(\"\\nDP parameter balancing:\")\n",
        "        results_bp_dp = optimized_simulation.balance_parameters_dp(\n",
        "            optimized_param_map,\n",
        "            k_max=15,\n",
        "            error_constraint=error_constraint,\n",
        "            excluded_processors=excluded_processors_for_balance\n",
        "        )\n",
        "        for k in results_bp_dp:\n",
        "            print(f\"\\nSets of size {k}:\")\n",
        "            total_sets = results_bp_dp[k]['sets']\n",
        "            for idx, combo in enumerate(total_sets):\n",
        "                names = combo['names']\n",
        "                total = combo['total']\n",
        "                print(f\"  Set {idx + 1}: {names}, Net Value: {total}\")\n",
        "            print(f\"  Sum of Positives: {results_bp_dp[k]['sum_positives']}\")\n",
        "            print(f\"  Sum of Negatives: {results_bp_dp[k]['sum_negatives']}\")\n",
        "    print(\"\\nSaving results to CSV files...\")\n",
        "    file_path01 = 'idle_combinations.csv'\n",
        "    optimized_simulation.save_joint_times_to_csv(output_file=file_path01)\n",
        "    file_path02 = 'clean_idle_combinations.csv'\n",
        "    optimized_simulation.clean_idle_combinations_csv(\n",
        "        input_file=file_path01,\n",
        "        output_file=file_path02,\n",
        "        excluded_processors=exclude_processors\n",
        "    )\n",
        "    all_processors = set()\n",
        "    for path_data in optimized_paths_info.values():\n",
        "        all_processors.update(path_data['processing_times'].keys())\n",
        "    all_processors = sorted(all_processors - set(exclude_processors))\n",
        "    k = 1\n",
        "    optimal_pools = optimized_simulation.construct_optimal_pools(\n",
        "        file_path02,\n",
        "        k,\n",
        "        1000000,\n",
        "        excluded_processors=['Start_Job', 'Units_Delivered']\n",
        "    )\n",
        "    file_path03 = 'optimal_pools.csv'\n",
        "    optimized_simulation.save_optimal_pools_to_csv(optimal_pools, output_file=file_path03)\n",
        "    file_path04 = 'processor_schedules_dependencies.csv'\n",
        "    optimized_simulation.save_processor_schedule_and_dependencies_to_csv(output_file=file_path04)\n",
        "    end_time = time.time()\n",
        "    runtime = end_time - start_time\n",
        "    print(f\"\\nRuntime: {runtime:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}